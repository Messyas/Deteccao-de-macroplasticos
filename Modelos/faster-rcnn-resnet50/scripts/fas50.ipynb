{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc7b47be-cddb-43d4-979c-185402f76b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.20s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.38s)\n",
      "creating index...\n",
      "index created!\n",
      "[E001|B0010/25808] loss=18.0398\n",
      "[E001|B0020/25808] loss=22.1314\n",
      "[E001|B0030/25808] loss=24.2947\n",
      "[E001|B0040/25808] loss=13.9815\n",
      "[E001|B0050/25808] loss=10.6287\n",
      "[E001|B0060/25808] loss=10.4198\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 202\u001b[0m\n\u001b[1;32m    200\u001b[0m best, wait, patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m30\u001b[39m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 202\u001b[0m     tr \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     vl \u001b[38;5;241m=\u001b[39m val_epoch(ep)\n\u001b[1;32m    204\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 168\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m    166\u001b[0m     loss \u001b[38;5;241m=\u001b[39m raw_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(imgs)\n\u001b[1;32m    167\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 168\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    170\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m2.0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tcc/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tcc/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tcc/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.transforms.functional import to_tensor, normalize, hflip, vflip\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import OneCycleLR, ReduceLROnPlateau\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "\n",
    "# 1) reproducibility & device\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2) dataset paths\n",
    "IMG_DIR   = \"/home/messyas/data/images\"\n",
    "ANN_TRAIN = \"/home/messyas/data/annotations/10_class_splits/annotations_train.json\"\n",
    "ANN_VAL   = \"/home/messyas/data/annotations/10_class_splits/annotations_val.json\"\n",
    "\n",
    "# 3) transforms\n",
    "class TrainTransform:\n",
    "    def __init__(self, base_size=512, scale_range=(500, 700), noise_std=0.02):  # narrower scales and less noise\n",
    "        self.base_size = base_size\n",
    "        self.scale_range = scale_range\n",
    "        self.noise_std = noise_std\n",
    "        self.mean = [0.485, 0.456, 0.406]\n",
    "        self.std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        # random scale\n",
    "        s = random.randint(self.scale_range[0], self.scale_range[1])\n",
    "        image = image.resize((s, s), Image.BILINEAR)\n",
    "        ow, oh = image.size\n",
    "        # horizontal flip\n",
    "        if random.random() < 0.5:\n",
    "            image = hflip(image)\n",
    "            for obj in target:\n",
    "                x, y, w, h = obj[\"bbox\"]\n",
    "                obj[\"bbox\"][0] = ow - x - w\n",
    "        # vertical flip\n",
    "        if random.random() < 0.5:\n",
    "            image = vflip(image)\n",
    "            for obj in target:\n",
    "                x, y, w, h = obj[\"bbox\"]\n",
    "                obj[\"bbox\"][1] = oh - y - h\n",
    "        # resize to base\n",
    "        image = image.resize((self.base_size, self.base_size), Image.BILINEAR)\n",
    "        tensor = to_tensor(image)\n",
    "        tensor = tensor + torch.randn_like(tensor) * self.noise_std\n",
    "        tensor = normalize(tensor, mean=self.mean, std=self.std)\n",
    "        # adjust boxes\n",
    "        sx = self.base_size / ow\n",
    "        sy = self.base_size / oh\n",
    "        for obj in target:\n",
    "            x, y, w, h = obj[\"bbox\"]\n",
    "            obj[\"bbox\"] = [x * sx, y * sy, w * sx, h * sy]\n",
    "        return tensor, target\n",
    "\n",
    "class EvalTransform:\n",
    "    def __init__(self, size=512):\n",
    "        self.size = size\n",
    "        self.mean = [0.485, 0.456, 0.406]\n",
    "        self.std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        ow, oh = image.size\n",
    "        image = image.resize((self.size, self.size), Image.BILINEAR)\n",
    "        tensor = to_tensor(image)\n",
    "        tensor = normalize(tensor, mean=self.mean, std=self.std)\n",
    "        sx = self.size / ow\n",
    "        sy = self.size / oh\n",
    "        for obj in target:\n",
    "            x, y, w, h = obj[\"bbox\"]\n",
    "            obj[\"bbox\"] = [x * sx, y * sy, w * sx, h * sy]\n",
    "        return tensor, target\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# 4) DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    CocoDetection(IMG_DIR, ANN_TRAIN, transforms=TrainTransform()),\n",
    "    batch_size=2, shuffle=True, num_workers=4, pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    CocoDetection(IMG_DIR, ANN_VAL, transforms=EvalTransform()),\n",
    "    batch_size=4, shuffle=False, num_workers=4, pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# 5) Model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "# custom anchors for small objects\n",
    "model.rpn.anchor_generator = AnchorGenerator(\n",
    "    sizes=((16,), (32,), (64,), (128,), (256,)),\n",
    "    aspect_ratios=((0.5, 1.0, 2.0),) * 5\n",
    ")\n",
    "# replace head\n",
    "in_feats = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_feats, 11)\n",
    "# dropout\n",
    "orig_cls = model.roi_heads.box_predictor.cls_score\n",
    "model.roi_heads.box_predictor.cls_score = nn.Sequential(nn.Dropout(0.3), orig_cls)\n",
    "model.to(device)\n",
    "\n",
    "# 6) Optimizer and schedulers\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)  # increased LR for faster convergence\n",
    "EPOCHS = 100\n",
    "steps_per_epoch = len(train_loader)\n",
    "onecycle = OneCycleLR(optimizer, max_lr=5e-5, total_steps=EPOCHS * steps_per_epoch, pct_start=0.1)\n",
    "plateau = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-6)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 7) Freeze backbone\n",
    "for p in model.backbone.body.parameters():\n",
    "    p.requires_grad = False\n",
    "unfreeze_epoch = 10\n",
    "backbone_unfrozen = False\n",
    "\n",
    "# 8) Target prep\n",
    "def to_valid(images, targets):\n",
    "    v_imgs, v_tgts = [], []\n",
    "    for img, tgt in zip(images, targets):\n",
    "        boxes, labels = [], []\n",
    "        for obj in tgt:\n",
    "            x, y, w, h = obj[\"bbox\"]\n",
    "            if w > 0 and h > 0:\n",
    "                boxes.append([x, y, x + w, y + h])\n",
    "                labels.append(int(obj[\"category_id\"]))\n",
    "        if boxes:\n",
    "            v_imgs.append(img.to(device))\n",
    "            v_tgts.append({\n",
    "                \"boxes\": torch.tensor(boxes, dtype=torch.float32, device=device),\n",
    "                \"labels\": torch.tensor(labels, dtype=torch.int64, device=device)\n",
    "            })\n",
    "    return v_imgs, v_tgts\n",
    "\n",
    "# 9) Train/val loops\n",
    "def train_epoch(epoch):\n",
    "    global backbone_unfrozen\n",
    "    model.train()\n",
    "    if epoch == unfreeze_epoch and not backbone_unfrozen:\n",
    "        for p in model.backbone.body.parameters(): p.requires_grad = True\n",
    "        backbone_unfrozen = True\n",
    "        print(f\"*** Backbone descongelado na Ã©poca {epoch} ***\")\n",
    "    running, count = 0.0, 0\n",
    "    for i, (imgs, tgts) in enumerate(train_loader, start=1):\n",
    "        imgs, tgts = to_valid(imgs, tgts)\n",
    "        if not tgts: continue\n",
    "        with autocast(\"cuda\"):\n",
    "            loss_dict = model(imgs, tgts)\n",
    "            raw_loss = sum(loss_dict.values())\n",
    "            # normalize by batch size\n",
    "            loss = raw_loss / len(imgs)\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "        step_ok = scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        if step_ok is not None:\n",
    "            onecycle.step()\n",
    "        running += loss.item()\n",
    "        count += 1\n",
    "        if i % 10 == 0:\n",
    "            print(f\"[E{epoch:03d}|B{i:04d}/{steps_per_epoch:04d}] loss={loss.item():.4f}\")\n",
    "    avg = running / count if count else float('nan')\n",
    "    print(f\"[Epoch {epoch:03d}] train avg loss: {avg:.4f}\\n\")\n",
    "    torch.cuda.empty_cache()\n",
    "    return avg\n",
    "\n",
    "def val_epoch(epoch):\n",
    "    model.train()\n",
    "    total, count = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, tgts in val_loader:\n",
    "            imgs, tgts = to_valid(imgs, tgts)\n",
    "            if not tgts: continue\n",
    "            with autocast(\"cuda\"):\n",
    "                total += sum(model(imgs, tgts).values()).item()\n",
    "            count += 1\n",
    "    avg = total / count if count else float('nan')\n",
    "    print(f\"[Epoch {epoch:03d}] val avg loss: {avg:.4f}\\n\")\n",
    "    plateau.step(avg)\n",
    "    return avg\n",
    "\n",
    "# 10) Main loop\n",
    "best, wait, patience = float('inf'), 0, 30\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr = train_epoch(ep)\n",
    "    vl = val_epoch(ep)\n",
    "    torch.save(model.state_dict(), f\"epoch_{ep:03d}.pth\")\n",
    "    if vl < best:\n",
    "        best, wait = vl, 0\n",
    "        torch.save(model.state_dict(), \"best.pth\")\n",
    "        print(\"++ best saved\\n\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "# 11) final save\n",
    "torch.save(model.state_dict(), \"final.pth\")\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d3247e-fe5a-48b3-985d-e3a79403307e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
